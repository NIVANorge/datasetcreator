{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% import\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dataclasses import  asdict\n",
    "from datetime import datetime, timedelta\n",
    "from dscreator.storage import get_storage_handler\n",
    "from dscreator.sources.ferrybox.extractor import TrajectoryExtractor\n",
    "from sqlalchemy import create_engine\n",
    "from dscreator.config import SETTINGS\n",
    "from dscreator.datasets.trajectories.ferrybox import NorsoopFantasy\n",
    "from dscreator.sources.ferrybox.uuid_variable_code_mapper import MAPPER\n",
    "import numpy as np\n",
    "\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_manual_qc(traj_raw: dict, boat: str, year: str) -> dict:\n",
    "    \"\"\"Helper to read and apply manually checked data to a named trajectory\n",
    "    \"\"\"\n",
    "    bad_ox = pd.read_csv(f'Norsoop-manual-qc-files/{boat}{year}_bad_ox_sat_dates.txt', header=None)\n",
    "    bad_T = pd.read_csv(f'Norsoop-manual-qc-files/{boat}{year}_bad_inletT_dates.txt', header=None)\n",
    "    bad_T = np.array([dt.to_pydatetime() for dt in pd.to_datetime(bad_T[bad_T.columns[0]]).to_list()])\n",
    "    bad_ox = np.array([dt.to_pydatetime() for dt in pd.to_datetime(bad_ox[bad_ox.columns[0]]).to_list()])\n",
    "    i_bad_T = np.intersect1d(np.array(traj_raw[\"time\"]), bad_T, return_indices=True)[1]\n",
    "    i_bad_ox = np.intersect1d(np.array(traj_raw[\"time\"]), bad_ox, return_indices=True)[1]\n",
    "    print(f\"Found {len(i_bad_T)} timestamp with no flow, and {len(i_bad_ox)} with bad oxygen_sat\")\n",
    "    print(f\"{len(np.intersect1d(i_bad_T, i_bad_ox))} timestamps overlap\")\n",
    "    print(f\"About to set values for oxygen to None for bad oxygen timestamps\")\n",
    "\n",
    "    traj_raw[\"oxygen_sat\"] = [None if i in i_bad_ox else val for i,val in enumerate(traj_raw[\"oxygen_sat\"])]\n",
    "\n",
    "    print(f\"About to remove bad flow data. Before removal size of data based on location is \"\n",
    "          f\"{len(traj_raw['longitude'])}\")\n",
    "    for k in traj_raw:\n",
    "        traj_raw[k] = np.delete(traj_raw[k], i_bad_T)\n",
    "    print(f\"After removal size of data based on location is {len(traj_raw['longitude'])}\")\n",
    "\n",
    "    return traj_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% input parameters\n"
    }
   },
   "outputs": [],
   "source": [
    "boat = \"FA\"\n",
    "measurement_parameters = [\"temperature\", \"salinity\", \"oxygen_sat\"]\n",
    "stationname=\"color_fantasy\"\n",
    "datasetname=stationname\n",
    "projectname=\"NorSoop\"\n",
    "\n",
    "tb = NorsoopFantasy(\n",
    "    uuid=\"no.niva:14bb8759-81d8-4a1a-948a-14219d374fab\",\n",
    "    dataset_name=datasetname,\n",
    "    station_name=stationname,\n",
    "    grouping=projectname,\n",
    "    is_acdd=False\n",
    ")\n",
    "years = [2017, 2018, 2019, 2020, 2021, 2022]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% fetch data\n"
    }
   },
   "outputs": [],
   "source": [
    "engine = create_engine(SETTINGS.tsb_connection_str)\n",
    "variable_uuid_map = MAPPER[f\"{boat}_19\"]\n",
    "traj_extractor = TrajectoryExtractor(engine, measurement_parameters, variable_uuid_map, [1])\n",
    "nc_paths = []\n",
    "for year in years:\n",
    "    print(f\"Extracting year {year}\")\n",
    "    if year == 2020:\n",
    "        variable_uuid_map = MAPPER[f\"{boat}_20\"]\n",
    "        traj_extractor = TrajectoryExtractor(engine, measurement_parameters, variable_uuid_map, [1])\n",
    "    elif year == 2022:\n",
    "        variable_uuid_map = MAPPER[f\"{boat}_22\"]\n",
    "        traj_extractor = TrajectoryExtractor(engine, measurement_parameters, variable_uuid_map, [1])\n",
    "\n",
    "    start_time = datetime(year, 1, 1, 0, 0, 0)\n",
    "    traj_raw = traj_extractor.fetch_slice(start_time=start_time, end_time=start_time + timedelta(days=365))\n",
    "    ds = tb.create(apply_manual_qc(traj_raw, boat, year))\n",
    "\n",
    "    # Set missing value flags\n",
    "    ds.temperature_qc[ds.temperature.isnull()] = 9\n",
    "    ds.salinity_qc[ds.salinity.isnull()] = 9\n",
    "    ds.oxygen_sat_qc[ds.oxygen_sat.isnull()] = 9\n",
    "    # Store each year on disk after extraction\n",
    "    SETTINGS.storage_path = os.path.join(os.getcwd(), \"..\", \"catalog\")\n",
    "    sh = get_storage_handler(\n",
    "        grouping=projectname,\n",
    "        dataset_name=datasetname,\n",
    "        unlimited_dims=[\"time\"],\n",
    "        filename_prefix= f\"FA_{year}\"\n",
    "    )\n",
    "    fname = sh.save_dataset(ds)\n",
    "    print(f\"Dumped {fname.split('/')[-1]}\")\n",
    "    nc_paths.append(fname)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge each year of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds = xr.merge([xr.open_dataset(p) for p in nc_paths])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "refresh attributes based on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.attrs = asdict(tb.dataset_attributes(ds))\n",
    "ds.attrs['id'] = tb.uuid\n",
    "ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flag data outside bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_mask = (ds.latitude >= ds.attrs[\"geospatial_lat_min\"]) & (ds.latitude <= ds.attrs[\"geospatial_lat_max\"])\n",
    "lon_mask = (ds.longitude >= ds.attrs[\"geospatial_lon_min\"]) & (ds.longitude <= ds.attrs[\"geospatial_lon_max\"])\n",
    "bounding_mask = (lon_mask&lat_mask)\n",
    "# Set flag on suspicious positions\n",
    "ds.temperature_qc[~bounding_mask] = 3\n",
    "ds.salinity_qc[~bounding_mask] = 3\n",
    "ds.oxygen_sat_qc[~bounding_mask] = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store dataset on object storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SETTINGS.storage_path = \"gs://nivaprod-1-senda\"\n",
    "sh = get_storage_handler(\n",
    "        grouping=projectname,\n",
    "        dataset_name=datasetname,\n",
    "        unlimited_dims=[\"time\"],\n",
    "        filename_prefix= f\"merged\"\n",
    ")\n",
    "sh.save_dataset(ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store local csv version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    ds.sel(time=str(year)).to_dataframe().to_csv(f\"{year}_acdd_color_fantasy.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
